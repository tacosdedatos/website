<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator>
  <link href="https://old.tacosdedatos.com/tag/texto/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://old.tacosdedatos.com/" rel="alternate" type="text/html" />
  <updated>2021-08-01T20:46:16+00:00</updated>
  <id>https://old.tacosdedatos.com/tag/texto/feed.xml</id>

  
  
  

  
    <title type="html">üåÆ tacos de datos | Aprende visualizaci√≥n de datos en espa√±ol. | </title>
  

  
    <subtitle>Tu sitio para aprender de visualizaci√≥n y ciencia de datos en espa√±ol. Consejos, recursos y mejores pr√°cticas para tus proyectos de tecnolog√≠a, periodismo de datos y an√°lisis estad√≠sticos.</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Word2vec ilustrado</title>
      <link href="https://old.tacosdedatos.com/word-to-vec-ilustrado" rel="alternate" type="text/html" title="Word2vec ilustrado" />
      <published>2020-09-04T10:00:00+00:00</published>
      <updated>2020-09-04T10:00:00+00:00</updated>
      <id>https://old.tacosdedatos.com/word-to-vec-ilustrado</id>
      <content type="html" xml:base="https://old.tacosdedatos.com/word-to-vec-ilustrado">&lt;p&gt;En esta ocasi√≥n les quiero hablar de otra forma de convertir texto a vectores, esta es distinta a las que hemos visto previamente ya que nos da como resultado un vector por cada token y cada uno de estos vectores es un vector denso.&lt;/p&gt;

&lt;p&gt;Esta vez les traigo no un post original, sino m√°s bien una traducci√≥n de un art√≠culo que me parece vale mucho la pena. El art√≠culo original es de Jay Alammar y se llama &lt;a href=&quot;http://jalammar.github.io/illustrated-word2vec/&quot;&gt;The Illustrated Word2vec&lt;/a&gt;:&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúHay en todas las cosas un ritmo que es parte de nuestro universo. Hay simetr√≠a, elegancia y gracia‚Ä¶ esas cualidades a las que se acoge el verdadero artista. Uno puede encontrar este ritmo en la sucesi√≥n de las estaciones, en la forma en que la arena modela una cresta, en las ramas de un arbusto creosota o en el dise√±o de sus hojas. 
Intentamos copiar este ritmo en nuestras vidas y en nuestra sociedad, buscando la medida y la cadencia que reconfortan. Y sin embargo, es posible ver un peligro en el descubrimiento de la perfecci√≥n √∫ltima. Est√° claro que el √∫ltimo esquema contiene en s√≠ mismo su propia fijeza. En esta perfecci√≥n, todo conduce hacia la muerte‚Äù ~ Frank Herbert. ‚ÄúDune‚Äù (1965)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Encuentro la idea de &lt;em&gt;embeddings&lt;/em&gt;* una de las m√°s fascinantes dentro del aprendizaje autom√°tico. Si alguna vez has usado &lt;em&gt;Siri&lt;/em&gt;, &lt;em&gt;Google Assistant&lt;/em&gt;, &lt;em&gt;Alexa&lt;/em&gt; o &lt;em&gt;Google Translate&lt;/em&gt;, o inclusive un tel√©fono con teclado que predice tu siguiente palabra, entonces seguramente te has beneficiado de esta idea que se a convertido en la clave de los modelos de Procesamiento de Lenguaje Natural (&lt;em&gt;NLP&lt;/em&gt;). En las √∫ltimas d√©cadas ha existido mucho desarrollo respecto a usar &lt;em&gt;embeddings&lt;/em&gt; para modelos neuronales (investigaciones recientes incluyen &lt;em&gt;embeddings&lt;/em&gt; contextualizados que llevan a modelos vanguardistas como &lt;a href=&quot;https://jalammar.github.io/illustrated-bert/&quot;&gt;BERT&lt;/a&gt; o GPT2).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Word2vec&lt;/strong&gt; es un m√©todo para crear &lt;em&gt;embeddings&lt;/em&gt; de forma eficiente que ha existido desde 2013. pero adem√°s de su utilidad para la creaci√≥n de estos &lt;em&gt;embeddings&lt;/em&gt;, algunos de sus conceptos han sido exitosamente empleados para crear modelos de recomendaci√≥n y para hacer sentido de datos secuenciales, inclusive en aplicaciones comerciales no relacionadas con lenguajes. Compa√±√≠as como &lt;a href=&quot;https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb&quot;&gt;Airbnb&lt;/a&gt;, &lt;a href=&quot;https://www.kdd.org/kdd2018/accepted-papers/view/billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba&quot;&gt;Alibaba&lt;/a&gt;, &lt;a href=&quot;https://www.slideshare.net/AndySloane/machine-learning-spotify-madison-big-data-meetup&quot;&gt;Spotify&lt;/a&gt;, y &lt;a href=&quot;https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484&quot;&gt;Anghami&lt;/a&gt; le han sacado provecho a esta brillante pieza del mundo del &lt;em&gt;NLP&lt;/em&gt; y la est√°n usando para potenciar una nueva clase de modelos de recomendaci√≥n.&lt;/p&gt;

&lt;p&gt;En este post, vamos a revisar el concepto de &lt;em&gt;embeddings&lt;/em&gt;, y c√≥mo es que se generan estos con la t√©cnica de &lt;em&gt;word2vec&lt;/em&gt;. Pero comencemos con un ejemplo para familiarizarnos con el uso de vectores para representar cosas. ¬øSab√≠as que una lista de cinco n√∫meros (es decir, un vector) puede representar mucho sobre tu personalidad?&lt;/p&gt;

&lt;h2 id=&quot;embeddings-de-personalidad-c√≥mo-eres&quot;&gt;&lt;em&gt;Embeddings&lt;/em&gt; de personalidad: ¬øc√≥mo eres?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúTe doy el camale√≥n del desierto, cuya habilidad para mezclarse con el fondo te dice todo lo que necesitas saber sobre las ra√≠ces de la ecolog√≠a y las bases de la identidad personal‚Äù ~ Hijos de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En una escala de 0 a 100, ¬øqu√© tan introvertido/extrovertido eres tu (donde 0 es introvertido, y 100 es extrovertido)? ¬øalguna vez has tomado un test de personalidad como MBTI ‚Äì o, mejor a√∫n, una prueba del &lt;a href=&quot;https://es.wikipedia.org/wiki/Modelo_de_los_cinco_grandes&quot;&gt;modelo de los cinco grandes&lt;/a&gt;? si no lo has hecho, este tipo de pruebas te hace una serie de preguntas y te califica en diferentes ejes, siendo la introversi√≥n o extraversi√≥n uno de ellos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/big-five-personality-traits-score.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Ejemplo de un resultado en una prueba del modelo de los cinco grandes. Este te puede decir mucho sobre ti mismo y ha mostrado tener habilidades predictivas en cuanto a √©xito &lt;a href=&quot;http://psychology.okstate.edu/faculty/jgrice/psyc4333/FiveFactor_GPAPaper.pdf&quot;&gt;acad√©mico&lt;/a&gt;, &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1744-6570.1999.tb00174.x&quot;&gt;personal&lt;/a&gt; y &lt;a href=&quot;https://www.massgeneral.org/psychiatry/assets/published_papers/soldz-1999.pdf&quot;&gt;profesional&lt;/a&gt;. &lt;a href=&quot;https://projects.fivethirtyeight.com/personality-quiz/&quot;&gt;Aqu√≠&lt;/a&gt; puedes calcular tus valores.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Imagina que obtuve 38/100 en el eje de introversi√≥n/extroversi√≥n. Lo podemos graficar as√≠:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/introversion-extraversion-100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Si colocamos los valores de -1 a 1:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/introversion-extraversion-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;¬øQu√© tan bien crees conocer a una persona si sabes solo esta informaci√≥n sobre ella? No mucho, las personas son complejas. A√±adamos otra dimensi√≥n, la calificaci√≥n de otra de las caracter√≠sticas del test:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/two-traits-vector.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Podemos representar dos dimensiones como un punto en la gr√°fica, o mejor a√∫n, como un vector desde el origen a ese punto. Tenemos herramientas incre√≠bles que para trabajar con vectores que nos resultar√°n √∫tiles m√°s adelante.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;He ocultado qu√© caracter√≠sticas estamos graficando solamente para que nos acostumbremos a no saber qu√© representa cada dimensi√≥n ‚Äì aun asi, estamos obteniendo mucha informaci√≥n de la representaci√≥n vectorial de cada una de las personalidades.&lt;/p&gt;

&lt;p&gt;Ahora podemos decir que este vector representa parcialmente mi personalidad. La usabilidad de esta representaci√≥n es √∫til cuando quieres comparar otras dos personas conmigo. Digamos que me atropella un autob√∫s y debo ser reemplazado por alguien con una personalidad similar. Dada la siguiente figura, ¬øcu√°l de las dos personas es m√°s similar a mi?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/personality-two-persons.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cuando estamos trabajando con vectores, una forma com√∫n de calcular una medida de similitud es la &lt;a href=&quot;https://es.wikipedia.org/wiki/Similitud_coseno&quot;&gt;similitud coseno&lt;/a&gt; (o &lt;em&gt;cosine similarity&lt;/em&gt; que es su nombre en ingl√©s).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/cosine-similarity.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;span style=&quot;color: #70BF41;&quot;&gt;Person #1&lt;/span&gt; es m√°s similar a mi en cuanto a personalidad. Vectores que apuntan a la misma direcci√≥n (aunque la longitud tambi√©n tiene que ver) tienen una similitud coseno m√°s grande.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Aun asi, dos dimensiones no son suficientes para capturar informaci√≥n suficiente sobre qu√© tan diferentes dos personas son. D√©cadas de investigaci√≥n psicol√≥gica han llevado a que existen 5 caracter√≠sticas (y muchas sub-caracter√≠sticas). As√≠ que vamos a usar todas en nuestras comparaciones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/big-five-vectors.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;El problema con estas cinco dimensiones es que hemos perdido la habilidad de graficar esas flechitas tan bonitas en dos dimensiones. Este es un obst√°culo com√∫n en el aprendizaje autom√°tico en el que constantemente tenemos que pensar en espacios de grandes dimensiones. La ventaja que teneos es que la similitud coseno a√∫n funciona sin importar las dimensiones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/embeddings-cosine-personality.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;La similitud coseno funciona sin importar el n√∫mero de dimensiones. Estas son mejores calificaciones porque han sido calculadas en una representaci√≥n con mayor resoluci√≥n de las cosas que est√°n siendo comparadas.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Para concluir esta secci√≥n, quiero que nos quedemos con dos ideas principales:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Podemos representar personas (y cosas) como vectores de n√∫meros (lo que es perfecto para las computadoras).&lt;/li&gt;
  &lt;li&gt;Podemos comparar f√°cilmente qu√© tan similares son los vectores entre s√≠.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/section-1-takeaway-vectors-cosine.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;embeddings-de-palabras&quot;&gt;&lt;em&gt;Embeddings&lt;/em&gt; de palabras&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúEl don de las palabras es el don del enga√±o y la ilusi√≥n‚Äù ~ Hijos de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Entendiendo esto, podemos proceder a ver ejemplos de vectores-palabra entrenados (tambi√©n conocidos como &lt;em&gt;embeddings&lt;/em&gt;) y ver algunas de sus propiedades interesantes.&lt;/p&gt;

&lt;p&gt;Este es un &lt;em&gt;embedding&lt;/em&gt; para la palabra &lt;em&gt;‚Äúking‚Äù&lt;/em&gt; ‚Äìrey, en ingl√©s‚Äì (GloVe vector entrenado en Wikipedia):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Es una lista de 50 n√∫meros. No podemos decir mucho si solamente vemos estos n√∫meros.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/king-white-embedding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A√±adamos colores a las celdas basados en sus valores (rojo si est√°n cerca de 2, blanco si est√°n cerca de 0 y azul si est√°n cerca de -2):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/king-colored-embedding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;De ahora en adelante, vamos a ignorar los n√∫meros y concentrarnos solo en los colores que indican los valores de las celdas. Comparemos &lt;em&gt;‚Äúking‚Äù&lt;/em&gt; contra otras palabras:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/king-man-woman-embedding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;¬øVes c√≥mo las palabras &lt;em&gt;‚Äúman‚Äù&lt;/em&gt; y &lt;em&gt;‚Äúwoman‚Äù&lt;/em&gt; son m√°s similares entre s√≠ que cualquiera de ellas con &lt;em&gt;‚Äúking‚Äù&lt;/em&gt;? Esto nos dice algo. Estas representaciones vectoriales capturan un poco la informaci√≥n/significado/asociaciones de estas palabras.&lt;/p&gt;

&lt;p&gt;Aqu√≠ hay otros ejemplos (compara las columnas verticalmente, buscando columas con colores similares):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://jalammar.github.io/images/word2vec/queen-woman-girl-embeddings.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Algunas cosas para destacar:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Hay una columna roja que coincide en todas las palabras. Las palabras son similares en esa dimensi√≥n (recuerda que no sabemos lo que significa cada dimensi√≥n).&lt;/li&gt;
  &lt;li&gt;Puedes ver c√≥mo &lt;em&gt;‚Äúwoman‚Äù&lt;/em&gt; y &lt;em&gt;‚Äúgirl‚Äù&lt;/em&gt; son similares en un mont√≥n de lugares. Lo mismo sucede con &lt;em&gt;‚Äúman‚Äù&lt;/em&gt; y &lt;em&gt;‚Äúboy‚Äù&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;‚Äúboy‚Äù&lt;/em&gt; y &lt;em&gt;‚Äúgirl‚Äù&lt;/em&gt; tambi√©n tienen lugares en donde coinciden, pero son lugares diferentes a &lt;em&gt;‚Äúwoman‚Äù&lt;/em&gt; o &lt;em&gt;‚Äúman‚Äù&lt;/em&gt;. ¬øSer√° que estas est√©n codificando una vaga definici√≥n de juventud? es posible.&lt;/li&gt;
  &lt;li&gt;Todas las palabras, excepto la √∫ltima representan personas. Agregu√©, por ejemplo, un objeto (&lt;em&gt;‚Äúwater‚Äù&lt;/em&gt;) para mostrar las diferencias entre categor√≠as. Por ejemplo, ¬øves esa columna azul fuerte a la derecha que se aten√∫a cuando llegamos al &lt;em&gt;embedding&lt;/em&gt; de &lt;em&gt;‚Äúwater‚Äù&lt;/em&gt;?&lt;/li&gt;
  &lt;li&gt;Hay otros lugares en donde &lt;em&gt;‚Äúking‚Äù&lt;/em&gt; y &lt;em&gt;‚Äúqueen‚Äù&lt;/em&gt; son diferentes de todas las dem√°s, ¬øser√° que estas diferencias codifiquen un concepto vago de realeza?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;analog√≠as&quot;&gt;Analog√≠as&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúLas palabras pueden llevar todo el peso que queramos. Todo lo que se requiere es un acuerdo tradici√≥n a partir de la cual  construir‚Äù ~ Dios emperador de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Los ejemplos m√°s usados que muestran una de las caracter√≠sticas m√°s incre√≠bles de los &lt;em&gt;embeddings&lt;/em&gt; es el concepto de analog√≠as. Podemos sumar y restar &lt;em&gt;embeddings&lt;/em&gt; y llegar a resultados interesantes. El ejemplo m√°s famoso es la f√≥rmula &lt;em&gt;‚Äúking‚Äù&lt;/em&gt; - &lt;em&gt;‚Äúman‚Äù&lt;/em&gt; + &lt;em&gt;‚Äúwoman‚Äù&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://jalammar.github.io/images/word2vec/king-man+woman-gensim.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Usando la biblioteca &lt;a href=&quot;https://radimrehurek.com/gensim/&quot;&gt;Gensim&lt;/a&gt; de Python, podemos sumar y restar vectores, y encontrar las palabras m√°s similares al vector resultante. La imagen muestra una lista de las palabras m√°s similares, cada una con su similitud coseno.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Podemos visualizar esta analog√≠a como lo hemos hecho anteriormente:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://jalammar.github.io/images/word2vec/king-analogy-viz.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;El vector resultante de &lt;em&gt;‚Äúking-man+woman‚Äù&lt;/em&gt; no coincide exactamente con &lt;em&gt;‚Äúqueen‚Äù&lt;/em&gt; es la m√°s cercana de las 400,000 que la contiene este &lt;em&gt;dataset&lt;/em&gt;.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Ahora que hemos revisado los &lt;em&gt;embeddings&lt;/em&gt;, aprendamos m√°s acerca del proceso para obtenerlos. Pero antes de que lleguemos a &lt;em&gt;word2vec&lt;/em&gt;, necesitamos conocer a su padre conceptual: los modelos de lenguaje neuronales.&lt;/p&gt;

&lt;h2 id=&quot;modelando-lenguajes&quot;&gt;Modelando lenguajes&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúEl profeta no se distrae con ilusiones del pasado, presente y futuro. &lt;strong&gt;La fijeza del lenguaje determina tales distinciones lineales.&lt;/strong&gt; Los profetas sostienen la llave de la cerradura en un idioma. 
Este no es un universo mec√°nico. La progresi√≥n lineal de los eventos la impone el observador. ¬øCausa y efecto? No es eso para nada. &lt;strong&gt;El profeta pronuncia palabras fat√≠dicas.&lt;/strong&gt; Vislumbras algo ‚Äúdestinado a ocurrir‚Äù pero el instante prof√©tico libera algo de portento y poder infinitos. El universo sufre un cambio fantasmal‚Äù ~ Dios emperador de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Si uno quisiera un ejemplo de una aplicaci√≥n que usa &lt;em&gt;NLP&lt;/em&gt;, uno de los mejores ser√≠a la predicci√≥n de la pr√≥xima palabra en el teclado de un tel√©fono. Es una caracter√≠stica que miles de millones de personas usan cientos de veces al d√≠a.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/swiftkey-keyboard.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;La predicci√≥n de la pr√≥xima palabra es una tarea que puede llevarse a cabo usando un &lt;em&gt;modelo de lenguage&lt;/em&gt; (&lt;em&gt;language model&lt;/em&gt;, en ingl√©s). Un modelo de lenguaje puede tomar una lista de palabras (digamos, dos), y tratar de predecir cu√°l es la que le seguir√≠a.&lt;/p&gt;

&lt;p&gt;En la captura de pantalla de arriba, podemos pensar que el modelo tom√≥ estas dos palabras en color verde (&lt;span style=&quot;color: #70BF41;&quot;&gt;Thou&lt;/span&gt;, &lt;span style=&quot;color: #70BF41;&quot;&gt;shalt&lt;/span&gt;) y regresa un conjunto de sugerencias (‚Äúnot‚Äù es la que ten√≠a la mayor probabilidad):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/thou-shalt-_.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Puedes pensar en el modelo como una caja negra:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/language_model_blackbox.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pero en la pr√°ctica, el modelo no solamente regresa como resultado una sola palabra. En realidad, entrega las probabilidades para todas las palabras que ‚Äúconoce‚Äù (el conjunto de todas las palabras que conoce se llama vocabulario, que pueden ir desde unas cuantas miles hasta millones de palabras). Es la responsabilidad del teclado encontrar las palabras con mayor probabilidad y presentarlas al usuario.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/language_model_blackbox_output_vector.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Los resultados de un modelo de lenguaje son probabilidades sobre todas las palabras que el modelo ‚Äúconoce‚Äù. En la imagen nos referimos a la probabilidad como porcentaje, pero ese 40% en realidad vale 0.4 en nuestro vector de salida.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Despu√©s de ser entrenado, los primeros modelos de lenguaje (&lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;Bengio 2003&lt;/a&gt;) calculaban la predicci√≥n en tres pasos:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/neural-language-model-prediction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;El primer paso es el m√°s relevante para nosotros porque en este post estamos hablando sobre los &lt;em&gt;embeddings&lt;/em&gt;. Uno de los resultados del proceso de entrenamiento es una matriz que contiene un &lt;em&gt;embedding&lt;/em&gt; por cada uno de los tokens en nuestro vocabulario. Cuando estamos prediciendo, simplemente buscamos los &lt;em&gt;embeddings&lt;/em&gt; de los tokens de entrad ay calculamos la predicci√≥n:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/neural-language-model-embedding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ahora vamos a aprender c√≥mo es que esta matriz de &lt;em&gt;embeddings&lt;/em&gt; es creada.&lt;/p&gt;

&lt;h2 id=&quot;entrenamiento-de-modelos-de-lenguaje&quot;&gt;Entrenamiento de modelos de lenguaje&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúUn proceso no se puede entender deteni√©ndolo. La comprensi√≥n debe moverse con el flujo del proceso, debe unirse y fluir con √©l.‚Äù ~Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Los modelos de lenguaje tienen una gran ventaja sobre otros modelos de &lt;em&gt;machine learning&lt;/em&gt;. Esa ventaja es que podemos entrenarlos usando texto ‚Äì del cual tenemos mucho. Piensa en todos los libros, art√≠culos, y otras formas de textos a nuestro al rededor. En contraste con otros modelos de aprendizaje autom√°tico que necesitan que la los datos sean preparados (y a veces obtenidos) espec√≠ficamente para ellos.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúConocer√°s una palabra por sus la compa√±√≠a que mantiene alrededor‚Äù ~ J.R. Firth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Las palabras obtienen sus &lt;em&gt;embeddings&lt;/em&gt; a partir de las palabras que aparecen a su alrededor. Esto funciona de la siguiente manera:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Obtenemos un mont√≥n de texto (digamos, todos los art√≠culos de Wikipedia), luego&lt;/li&gt;
  &lt;li&gt;tomamos una ventana (digamos, de tres palabras) que movemos sobre todo el texto,&lt;/li&gt;
  &lt;li&gt;Esta ventana genera nuestros ejemplos para el entrenamiento del modelo:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/wikipedia-sliding-window.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;En tanto esta ventana se desliza, nosotros (virtualmente) generamos un &lt;em&gt;dataset&lt;/em&gt; que usaremos para entrenar el modelo. Para ver c√≥mo es que esto funciona, veamos c√≥mo funciona el proceso para la siguiente frase:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúThou shalt not make a machine in the likeness of a human mind‚Äù ~Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cuando empezamos, la ventana est√° en las tres primeras palabras de la oraci√≥n:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/lm-sliding-window.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tomamos las dos palabras como &lt;em&gt;features&lt;/em&gt;, y la tercera como la etiqueta a predecir:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/lm-sliding-window-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Luego entonces deslizamos la ventana a la siguiente posici√≥n para generar un segundo ejemplo de entrenamiento:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/lm-sliding-window-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Y de pronto tendremos un gran &lt;em&gt;dataset&lt;/em&gt; de palabras que suelen aparecer despu√©s de otro par:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/lm-sliding-window-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;En la pr√°ctica, los modelos suelen ser entrenados mientras esta ventana se va deslizando, sin embargo, siento que es m√°s claro separar l√≥gicamente la etapa de generaci√≥n del &lt;em&gt;dataset&lt;/em&gt; de la etapa de entrenamiento. Adem√°s de modelos basados en redes neuronales, existe una t√©cnica conocida como &lt;em&gt;n-grams&lt;/em&gt; que es tambi√©n usada com√∫nmente para entrenar modelos (mira el cap√≠tulo 3 de &lt;a href=&quot;http://web.stanford.edu/~jurafsky/slp3/&quot;&gt;&lt;em&gt;Speech and Language Processing&lt;/em&gt;&lt;/a&gt;). para ver c√≥mo es que este cambio de &lt;em&gt;n-grams&lt;/em&gt; a modelos neuronales se refleja en productos reales, revisa &lt;a href=&quot;https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/&quot;&gt;este post de Swiftkey&lt;/a&gt; mi teclado favorito para Android, introduciendo su modelo neuronal de lenguaje y compar√°ndolo con su previo modelo basado en &lt;em&gt;n-grams&lt;/em&gt;. me gusta este ejemplo porque muestra c√≥mo las propiedades algor√≠tmicas de los &lt;em&gt;embeddings&lt;/em&gt; se pueden describir en lenguaje de marketing.&lt;/p&gt;

&lt;h3 id=&quot;mira-hacia-ambos-lados&quot;&gt;Mira hacia ambos lados&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúLa paradoja es un indicador que te dice que mires m√°s all√°. Si las paradojas te molestan, eso delata tu profundo deseo de absolutos. El relativista trata una paradoja simplemente como interesante, quiz√°s divertida o incluso, un pensamiento terrible, educativo‚Äù¬†~ Dios emperador de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sabiendo de lo que hablamos anteriormente en este post, completa la oraci√≥n&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/jay_was_hit_by_a_.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;El contexto que te he dado aqu√≠ son 5 palabras antes del espacio en blanco. Estoy seguro que la mayor√≠a de las personas elegir√≠an la palabra &lt;em&gt;‚Äúbus‚Äù&lt;/em&gt; como soluci√≥n. Pero qu√© suceder√≠a si te doy un poco m√°s de informaci√≥n ‚Äì una palabra despu√©s del espacio en blanco, ¬øcambiar√≠a tu respuesta?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/jay_was_hit_by_a_.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esto cambia completamente lo que deber√≠a ir en el espacio en blanco, la palabra &lt;em&gt;‚Äúred‚Äù&lt;/em&gt; es ahora la que tiene mayor sentido de ser elegida. Lo que hemos aprendido de esto es que tanto las palabras previas como las siguientes a una palabra determinada contienen un alto valor sobre esta palabra determinada. Resulta que tomar en cuenta ambas direcciones (palabras a la izquierda y derecha de la que estamos adivinando) nos lleva a tener mejor &lt;em&gt;embeddings&lt;/em&gt;. Veamos c√≥mo es que podemos tomar en cuenta esto al momento de entrenar nuestro modelo.&lt;/p&gt;

&lt;h2 id=&quot;skipgram&quot;&gt;&lt;em&gt;Skipgram&lt;/em&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúLa inteligencia se arriesga con datos limitados en un campo donde los errores no solo son posibles sino tambi√©n necesarios.‚Äù ~Chapterhouse: Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En lugar de solamente mirar dos palabras antes de nuestra palabra objetivo, podemos tambi√©n ver dos palabras despu√©s:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/continuous-bag-of-words-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Si hacemos esto, el &lt;em&gt;dataset&lt;/em&gt; que estamos construyendo virtualmente para entrenar el modelo se ver√≠a as√≠:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/continuous-bag-of-words-dataset.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esta arquitectura es conocida como &lt;em&gt;Continuous Bag of Words&lt;/em&gt; y es descrita en uno de los &lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;art√≠culos de word2vec&lt;/a&gt;. Hay otra arquitectura que entreg√≥ grandes resultados haciendo las cosas un poco diferente.&lt;/p&gt;

&lt;p&gt;El lugar de ‚Äúadivinar‚Äù una palabra a partir de su contexto (las palabras antes y despu√©s de ella), esta otra arquitectura trata de predecir las palabras vecinas a partir de una palabra determinada. Podemos imaginar que la ventana se desliza sobre el texto de entrenamiento as√≠:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;La palabra en la casilla verde ser√° tratada como la palabra de entrada y cada una de las casillas rosas ser√°n tratadas como posibles resultados.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Las casillas rosas est√°n marcadas con diferentes tonalidades porque la ventana deslizante en realidad crea cuatro ejemplos diferentes en nuestro dataset:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este m√©todo es conocido como la arquitectura &lt;em&gt;skipgram&lt;/em&gt;. Podemos visualizar la ventana deslizante haciendo algo as√≠:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esto a√±adir√≠a cuatro ejemplos a nuestro dataset de entrenamiento:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Luego entonces podemos deslizar la ventana a su siguiente posici√≥n:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esto genera otros cuatro ejemplos:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Un par de posiciones m√°s adelante tenemos muchos m√°s ejemplos:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-sliding-window-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;revisando-el-proceso-de-entrenamiento&quot;&gt;Revisando el proceso de entrenamiento&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúMuad‚ÄôDib aprendi√≥ r√°pidamente porque su primer entrenamiento fue sobre c√≥mo aprender. Y la primera lecci√≥n de todas fue la confianza b√°sica que pod√≠a aprender. Es impactante descubrir cu√°ntas personas no creen que puedan aprender y cu√°ntas m√°s creen que aprender es dif√≠cil.‚Äù ~ Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ahora que ya tenemos nuestro &lt;em&gt;dataset&lt;/em&gt; creado a partir del modelo &lt;em&gt;skipgram&lt;/em&gt;, echemos un vistazo a c√≥mo lo podemos usar para entrenar un modelo neuronal b√°sico de lenguaje que predice las palabras vecinas a otra.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-language-model-training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Comencemos por el primer ejemplo en nuestro &lt;em&gt;dataset&lt;/em&gt;. Tomando la primer entrada y d√°ndosela al modelo que a√∫n no est√° entrenado pidi√©ndole su predicci√≥n para la siguiente palabra.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-language-model-training-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;El modelo ejecuta los tres pasos definidos arriba y entrega un vector de predicci√≥n (en donde cada palabra en su vocabulario recibe una probabilidad). Dado que el modelo no est√° entrenado a√∫n, sus predicciones son incorrectas en esta etapa; eso est√° bien. Nosotros sabemos qu√© palabra debi√≥ haber predicho ‚Äì la palabra (o ‚Äúetiqueta‚Äù) en la fila que estamos usando para entrenar el modelo:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-language-model-training-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;El vector objetivo (&lt;em&gt;‚Äútarget vector‚Äù&lt;/em&gt;) es aquel en donde la verdadera palabra esperada tiene una probabilidad de 1 mientras que cualquier otra tienen probabilidad 0.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;¬øQu√© tan lejos estuvo el modelo? para saber esto, restamos los dos vectores (el valor esperado menos el valor predecido) lo cual nos va a dar un vector ‚Äúerror‚Äù:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-language-model-training-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este vector ‚Äúerror‚Äù puede ser usado para actualizar el modelo para que, la siguiente vez que se le pregunte, sea m√°s probable que ‚Äúadivine‚Äù &lt;span style=&quot;color: #e91e63;&quot;&gt;thou&lt;/span&gt; cuando recibe &lt;span style=&quot;color: #4caf50;&quot;&gt;not&lt;/span&gt; como entrada.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-language-model-training-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Y as√≠ concluye el primer paso del entrenamiento. Procedemos a ejecutar el mismo proceso con el siguiente ejemplo en nuestro dataset, y el siguiente, y el siguiente y as√≠ hasta que hayamos terminado de cubrir todos los ejemplos de nuestro &lt;em&gt;dataset&lt;/em&gt;, con eso se cubre lo que en &lt;em&gt;machine learning&lt;/em&gt; se conoce como una &lt;strong&gt;√©poca de entrenamiento&lt;/strong&gt;. Despu√©s repetimos el mismo proceso por un n√∫mero de √©pocas y finalmente podemos extraer la matriz de &lt;em&gt;embeddings&lt;/em&gt; (que son los par√°metros internos de nuestro modelo neuronal) y usarla para cualquier otra aplicaci√≥n.&lt;/p&gt;

&lt;p&gt;Mientras qu√© hemos logrado entender el proceso, a√∫n no llegamos a c√≥mo es que &lt;em&gt;word2vec&lt;/em&gt; fue entrenado en realidad. Nos faltan un par de ideas claves.&lt;/p&gt;

&lt;h2 id=&quot;sampleo-negativo&quot;&gt;Sampleo negativo&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúIntentar comprender a Muad‚ÄôDib sin comprender a sus enemigos mortales, los Harkonnen, es intentar ver la Verdad sin conocer la Falsedad. Es el intento de ver la Luz sin conocer la Oscuridad. No puede ser.‚Äù ~ Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recordemos los tres pasos de c√≥mo es que este modelo neuronal calcula su predicci√≥n:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/language-model-expensive.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este tercer paso es muy costoso desde un punto de vista computacional ‚Äì especialmente sabiendo que tenemos que hacerlo una vez por cada ejemplo en nuestro dataset (que f√°cilmente ser√° millones de veces). Necesitamos hacer algo para mejorar el desempe√±o.&lt;/p&gt;

&lt;p&gt;Una forma de hacerlo es dividiendo nuestro objetivo en dos etapas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generar &lt;em&gt;embeddings&lt;/em&gt; de alta calidad (sin preocuparnos por predecir la siguiente palabra)&lt;/li&gt;
  &lt;li&gt;Usar estos &lt;em&gt;embeddings&lt;/em&gt; para entrenar un modelo de lenguaje (para ahora si, predecir la siguiente palabra)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nos vamos a enfocar en el paso 1, ya que este post se trata de &lt;em&gt;embeddings&lt;/em&gt;. Para generar unos de alta calidad mientras que usamos un modelo de alto desempe√±o podemos cambiar la funcionalidad del modelo de predecir la siguiente palabra:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/predict-neighboring-word.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A uno que tome las dos palabras (la de entrada y la que ser√≠a de salida) y regrese una medida indicando si estas palabras son vecinas o no (0 para ‚Äúno vecinas‚Äù, 1 para ‚Äúvecinas‚Äù):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/are-the-words-neighbors.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este simple cambio tambi√©n significa que podemos cambiar nuestro modelo de salidas de m√∫ltiples salidas a un modelo de regresi√≥n lineal ‚Äì que se convierte en uno m√°s sencillo y f√°cil de entrenar.&lt;/p&gt;

&lt;p&gt;Este cambio tambi√©n requiere que nosotros cambiemos la estructura de nuestro &lt;em&gt;dataset&lt;/em&gt; ‚Äì lo que era antes nuestra etiqueta ahora es otra entrada al modelo, y el valor a predecir es 0 o 1. Por ahora todos ser√°n 1 puesto que todas nuestras palabras son ‚Äúvecinas‚Äù.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-dataset.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este problema puede ser resuelto a una velocidad impresionante ‚Äì procesando millones de ejemplos en minutos; sin embargo, existe un peque√±o problema que debemos solucionar. Si todos nuestros ejemplos son positivos (es decir, etiqueta 1), nos estamos exponiendo a que nuestro modelo se pase de listillo y siempre regrese 1 como respuesta ‚Äì logrando 100% de precisi√≥n pero sin aprender nada (y en el proceso generar &lt;em&gt;embeddings&lt;/em&gt; que no sirven).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-smartass-model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para resolver esto, necesitamos incluir &lt;em&gt;ejemplos negativos&lt;/em&gt; en nuestro &lt;em&gt;dataset&lt;/em&gt; ‚Äì ejemplos de palabras que no tienen relaci√≥n y para las cuales nuestro modelo debe regresar 0 como predicci√≥n. Con eso tenemos ahora un verdadero reto para el cual nuestro modelo tiene que trabajar para resolver, sin embargo este proceso sigue siendo r√°pido.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-negative-sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Para cada ejemplo en nuestro &lt;em&gt;dataset&lt;/em&gt;, a√±adimos un ejemplo negativo. Estos tienen la misma palabra de ‚Äúentrada‚Äù y 0 como etiqueta.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Pero, ¬øqu√© colocamos como palabras de ‚Äúsalida‚Äù? pues podemos elegir palabras de nuestro vocabulario aleatoriamente:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-negative-sampling-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Esta idea est√° inspirada por &lt;a href=&quot;http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf&quot;&gt;&lt;em&gt;noise-constrative estimation&lt;/em&gt;&lt;/a&gt;. Nosotros estamos contrastando la verdadera se√±al (los ejemplos positivos de palabras vecinas) con ruido (los ejemplos de palabras que no son vecinas elegidos aleatoriamente). Esto representa un excelente balance entre eficiencia computacional y estad√≠stica.&lt;/p&gt;

&lt;h2 id=&quot;skipgram-con-sampleo-negativo&quot;&gt;&lt;em&gt;Skipgram&lt;/em&gt; con sampleo negativo&lt;/h2&gt;

&lt;p&gt;Ahora hemos cubierto dos de las ideas centrales en &lt;em&gt;word2vec&lt;/em&gt;: en conjunto son llamadas &lt;em&gt;skipgram&lt;/em&gt; con sampleo negativo (*Skipgram with Negative Sampling, SGNS)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/skipgram-with-negative-sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proceso-de-entrenamiento-de-word2vec&quot;&gt;Proceso de entrenamiento de &lt;em&gt;Word2vec&lt;/em&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúLa computadora no puede anticipar todos los problemas de importancia para los humanos. Es la diferencia entre bits en serie y un continuo ininterrumpido. Nosotros tenemos este; las m√°quinas se limitan al otro.‚Äù ~ Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Antes de que el proceso de entrenamiento comience, tenemos que pre-procesar el texto que vamos a usar para entrenar el modelo. Por ejemplo, determinamos el tama√±o de nuestro vocabulario (llamaremos a este valor &lt;span style=&quot;color:#ffa000;&quot;&gt;vocab_size&lt;/span&gt;, y digamos que su valor es 10,000) y qu√© palabras pertenecen a este.&lt;/p&gt;

&lt;p&gt;Al principio de la face de entrenamiento creamos dos matrices ‚Äì una de &lt;span style=&quot;color: #4caf50;&quot;&gt;Embedding&lt;/span&gt; y otra de &lt;span style=&quot;color: #9c27b0;&quot;&gt;Contexto&lt;/span&gt;. Estas dos matrices tienen un vector (&lt;em&gt;embedding&lt;/em&gt;) para cada palabra en el vocabulario, es decir &lt;span style=&quot;color:#ffa000;&quot;&gt;vocab_size&lt;/span&gt; es el tama√±o de una de sus dimensiones. La segunda dimensi√≥n es qu√© tan largo queremos que el &lt;em&gt;embedding&lt;/em&gt; sea (llamemos a este par√°metro &lt;span style=&quot;color: #ff6f00;&quot;&gt;embedding_size&lt;/span&gt;), 300 es un valor com√∫n, aunque anteriormente vimos un ejemplo de 50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-embedding-context-matrix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cuando el proceso de entrenamiento comienza, rellenamos estas matrices con valores aleatorios. En cada paso de entrenamiento, tomamos un ejemplo positivo y su correspondiente negativo. Ve√°moslo con el primer grupo:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ahora tenemos cuatro palabras: la de entrada &lt;span style=&quot;color: #4caf50;&quot;&gt;not&lt;/span&gt; y la de salida o contexto &lt;span style=&quot;color: #9c27b0;&quot;&gt;thou&lt;/span&gt; (que es su vecina). Adem√°s, tenemos &lt;span style=&quot;color: #9c27b0;&quot;&gt;aaron&lt;/span&gt; y &lt;span style=&quot;color: #9c27b0;&quot;&gt;taco&lt;/span&gt; como ejemplos negativos. Con esto en mente, ubicamos sus &lt;em&gt;embeddings&lt;/em&gt;: para la palabra de entrada, buscamos en la matriz &lt;span style=&quot;color: #4caf50;&quot;&gt;Embedding&lt;/span&gt; mientras que para las palabras ‚Äúcontexto‚Äù los buscamos en la matriz &lt;span style=&quot;color: #9c27b0;&quot;&gt;Contexto&lt;/span&gt; (a pesar de que ambas matrices tienen un &lt;em&gt;embedding&lt;/em&gt; para cada palabra en nuestro vocabulario).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-lookup-embeddings.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Despu√©s, tomamos el producto punto (&lt;em&gt;dot product&lt;/em&gt;) del &lt;em&gt;embedding&lt;/em&gt; de entrada con los &lt;em&gt;embeddings&lt;/em&gt; contexto. En cada caso, este producto punto resultar√° en un n√∫mero que indica la similtud entre estas dos palabras:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-dot-product.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Necesitamos una forma de convertir estos valores en algo que parezca probabilidades ‚Äì necesitamos que todas sean positivas y con valores entre 0 y 1. Este es un buen lugar para usar &lt;a href=&quot;https://jalammar.github.io/feedforward-neural-networks-visual-interactive/#sigmoid-visualization&quot;&gt;&lt;em&gt;sigmoid&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-dot-product-sigmoid.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ahora podemos tratar la salida de la operaci√≥n &lt;em&gt;sigmoid&lt;/em&gt; como la salida del modelo para estos ejemplos. Puedes ver que &lt;span style=&quot;color: #9c27b0;&quot;&gt;taco&lt;/span&gt; tiene el valor mayor mientras que &lt;span style=&quot;color: #9c27b0;&quot;&gt;aaron&lt;/span&gt; el menor, ambos antes y despu√©s de la operaci√≥n &lt;em&gt;sigmoid&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Una vez que el modelo no entrenado ha hecho una predicci√≥n, y sabiendo que tenemos un resultado correcto contra el cual comparar, vamos a calcular cu√°l fue el error en las predicciones del modelo. Para hacer eso, basta con restar los valores obtenidos de &lt;em&gt;sigmoid&lt;/em&gt; contra el valor de salida verdadero (&lt;em&gt;target&lt;/em&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aqu√≠ es donde viene la parte del &lt;em&gt;learning&lt;/em&gt; en &lt;em&gt;machine learning&lt;/em&gt;. Una vez que calculamos el error, podemos usarlo para ajustar los embeddings de nuestrass palabras, para que la pr√≥xima vez que necesitemos una predicci√≥n, los valores predecidos sean m√°s parecidos a los valores esperados.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-update.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Con esto concluye un primer paso de entrenamiento. Una vez concluido, terminamos con &lt;em&gt;embeddings&lt;/em&gt; un poquito mejores para las palabras involucradas en √©l (&lt;span style=&quot;color: #4caf50;&quot;&gt;not&lt;/span&gt;, &lt;span style=&quot;color: #9c27b0;&quot;&gt;thou&lt;/span&gt;, &lt;span style=&quot;color: #9c27b0;&quot;&gt;aaron&lt;/span&gt; y &lt;span style=&quot;color: #9c27b0;&quot;&gt;taco&lt;/span&gt;). Ahora si, podemos pasar al siguiente paso, es decir, el siguiente ejemplo positivo y sus correspondientes negativos para ejecutar el proceso nuevamente.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-training-example-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Los &lt;em&gt;embeddings&lt;/em&gt; seguir√°n mejorando mientras iteramos sobre todo nuestro &lt;em&gt;dataset&lt;/em&gt;. En cualquier momento podemos detener el proceso de entrenamiento, descartar la matriz &lt;span style=&quot;color: #9c27b0;&quot;&gt;Contexto&lt;/span&gt; y usar los &lt;em&gt;embeddings&lt;/em&gt; en la matriz &lt;span style=&quot;color: #4caf50;&quot;&gt;Embedding&lt;/span&gt; para cualquier otra tarea.&lt;/p&gt;

&lt;h2 id=&quot;tama√±o-de-ventana-y-n√∫mero-de-ejemplos-negativos&quot;&gt;Tama√±o de ventana y n√∫mero de ejemplos negativos&lt;/h2&gt;

&lt;p&gt;Hay dos h√≠per par√°metros claves en el proceso de entrenamiento de &lt;em&gt;word2vec&lt;/em&gt;, estos son el tama√±o de la ventana y el n√∫mero de ejemplos negativos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-window-size.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Diferentes tareas se benefician de diferentes tama√±os de ventana. Una &lt;a href=&quot;https://youtu.be/tAxrlAVw-Tk?t=648&quot;&gt;heur√≠stica&lt;/a&gt; que podemos emplear es que tama√±os de ventana peque√±os (2-15) llevan a &lt;em&gt;embeddings&lt;/em&gt; donde altos valores de similitud indican que las palabras son intercambiables (toma en cuenta que los ant√≥nimos suelen ser intercambiables si solo nos fijamos en las palabras que los rodean ‚Äì por ejemplo, ‚Äúbueno‚Äù y ‚Äúmalo‚Äù suelen aparecer en contextos similares). Los tama√±os de ventana m√°s grandes nos llevana a &lt;em&gt;embeddings&lt;/em&gt; en donde la similitud es m√°s bien una medida del nivel de relaci√≥n entre dos palabras. En la pr√°ctica, probablemente tengas que proveer &lt;a href=&quot;https://youtu.be/ao52o9l6KGw?t=287&quot;&gt;anotaciones&lt;/a&gt; que gu√≠en el proceso de generaci√≥n de &lt;em&gt;embeddings&lt;/em&gt; y entreguen un sentido mejor de similitud. El tama√±o de ventana por default en &lt;em&gt;Gensim&lt;/em&gt; es 5 (dos palabras antes y dos palabras despu√©s de la palabra de entrada).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/word2vec/word2vec-negative-samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;El n√∫mero de ejemplos negativos es otro factor a considerar en el proceso de entrenamiento, el &lt;em&gt;paper&lt;/em&gt; original recomienda de 5 a 20 como un buen n√∫mero de ejemplos negativos. Tambi√©n menciona que de 2 a 5 suele ser suficiente cuando se tiene un &lt;em&gt;dataset&lt;/em&gt; de buen tama√±o. El default en Gensim es 5 ejemplos negativos.&lt;/p&gt;

&lt;h2 id=&quot;conclusi√≥n&quot;&gt;Conclusi√≥n&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ÄúSi sale de tu dominio, entonces est√°s interactuando con inteligencia, no con automatizaci√≥n.‚Äù ~ Dios Emperador de Dune&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Espero que ahora ya comprendas qu√© son los &lt;em&gt;embeddings&lt;/em&gt; y el algoritmo detr√°s de &lt;em&gt;word2vec&lt;/em&gt;. Tambi√©n espero que cuando leas un art√≠culo cient√≠fico mencionando &lt;em&gt;‚Äúskipgram with negative sampling‚Äù&lt;/em&gt; (como los que mencion√© al principio), entiendas lo que significan estos conceptos. Como siempre, cualquier comentario es bien apreciado &lt;a href=&quot;https://twitter.com/JayAlammar&quot;&gt;@JayAlammar&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;referencias-y-recursos-para-seguir-leyendo&quot;&gt;Referencias y recursos para seguir leyendo&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt; [pdf]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt; [pdf]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A Neural Probabilistic Language Model&lt;/a&gt; [pdf]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt; by Dan Jurafsky and James H. Martin is a leading resource for NLP. Word2vec is tackled in Chapter 6.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984&quot;&gt;Neural Network Methods in Natural Language Processing&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/yoavgo&quot;&gt;Yoav Goldberg&lt;/a&gt; is a great read for neural NLP topics.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mccormickml.com/&quot;&gt;Chris McCormick&lt;/a&gt; has written some great blog posts about Word2vec. He also just released &lt;a href=&quot;https://www.preview.nearist.ai/paid-ebook-and-tutorial&quot;&gt;The Inner Workings of word2vec&lt;/a&gt;, an E-book focused on the internals of word2vec.&lt;/li&gt;
  &lt;li&gt;Want to read the code? Here are two options:
    &lt;ul&gt;
      &lt;li&gt;Gensim‚Äôs &lt;a href=&quot;https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py&quot;&gt;python implementation&lt;/a&gt; of word2vec&lt;/li&gt;
      &lt;li&gt;Mikolov‚Äôs original &lt;a href=&quot;https://github.com/tmikolov/word2vec/blob/master/word2vec.c&quot;&gt;implementation in C&lt;/a&gt; ‚Äì better yet, this &lt;a href=&quot;https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2vec.c&quot;&gt;version with detailed comments&lt;/a&gt; from Chris McCormick.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sro.sussex.ac.uk/id/eprint/61062/1/Batchkarov,%20Miroslav%20Manov.pdf&quot;&gt;Evaluating distributional models of compositional semantics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ruder.io/word-embeddings-1/index.html&quot;&gt;On word embeddings&lt;/a&gt;, &lt;a href=&quot;http://ruder.io/word-embeddings-softmax/&quot;&gt;part 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Dune-Frank-Herbert/dp/0441172717/&quot;&gt;Dune&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;small&gt;Alammar, Jay (2019). The Illustrated Word2vec [Blog post]. Retrieved from &lt;a href=&quot;http://jalammar.github.io/illustrated-word2vec/&quot;&gt;http://jalammar.github.io/illustrated-word2vec/&lt;/a&gt; &lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Sin m√°s les recuerdo que como siempre, quedo atento a sus dudas y comentarios en mi cuenta de Twitter &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;, en donde me pueden con contactar para hablar sobre ciencia de datos, ingenier√≠a de software y muchas cosas m√°s.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Antonio Feregrino Bola√±os</name>
        
        
      </author>

      

      
        <category term="python" />
      
        <category term="skelarn" />
      
        <category term="texto" />
      
        <category term="word2vec" />
      

      
        <summary type="html">En esta ocasi√≥n les quiero hablar de otra forma de convertir texto a vectores, esta es distinta a las que hemos visto previamente ya que nos da como resultado un vector por cada token y cada uno de estos vectores es un vector denso.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">De texto a vectores (parte 1)</title>
      <link href="https://old.tacosdedatos.com/texto-vectores" rel="alternate" type="text/html" title="De texto a vectores (parte 1)" />
      <published>2020-08-24T10:00:00+00:00</published>
      <updated>2020-08-24T10:00:00+00:00</updated>
      <id>https://old.tacosdedatos.com/texto-vectores</id>
      <content type="html" xml:base="https://old.tacosdedatos.com/texto-vectores">&lt;p&gt;Tener nuestro texto convertido en tokens es un paso importante en el uso de texto para aplicaciones de machine learning, existe una transformaci√≥n que debemos realizar para facilitarle la tarea a nuestros modelos. Esta transformaci√≥n es conocida como vectorizaci√≥n.&lt;/p&gt;

&lt;p&gt;En el &lt;a href=&quot;https://tacosdedatos.com/analisis-texto&quot;&gt;post pasado&lt;/a&gt; nos quedamos con nuestro texto ya tokenizado, sin embargo los modelos de machine learning operan con valores num√©ricos organizados en arreglos llamados vectores, y hasta el momento nuestros tokens son solo secuencias de caracteres. Nuestra tarea, y de lo que les voy a hablar en el siguiente post, es convertir esta secuencias a vectores.&lt;/p&gt;

&lt;h2 id=&quot;un-poco-de-nomenclatura&quot;&gt;Un poco de nomenclatura&lt;/h2&gt;
&lt;p&gt;Cuando se habla de texto en el contexto de la ciencia de datos hay algunas palabras que debemos entender ya que son com√∫nmente usadas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Token&lt;/strong&gt;, conjunto de caracteres que representa la m√≠nima unidad en el an√°lisis de texto.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Documento&lt;/strong&gt;, la representaci√≥n escrita de una idea, concepto o di√°logo, un documento est√° compuesto por varios tokens. Como ejemplos de documentos podemos tener un &lt;em&gt;tweet&lt;/em&gt;, un di√°logo en una pel√≠cula o un &lt;em&gt;paper&lt;/em&gt; de una publicaci√≥n cient√≠fica.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Corpus&lt;/strong&gt;, todo el conjunto de documentos sobre el que estamos realizando el an√°lisis.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vocabulario&lt;/strong&gt;, el conjunto de tokens √∫nicos que obtenemos como resultado al tokenizar nuestro &lt;em&gt;corpus&lt;/em&gt; completo.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por ejemplo, debajo tenemos tres documentos, que en conjunto forman nuestro corpus.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fui a comer tacos de suadero. Juro que es el suadero m√°s delicioso de mi vida. #suadero&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Taco de delicioso suadero con bolsa de pl√°stico para que no ensucie el plato.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tengo ganas de comprarme unos tacos de f√∫tbol, ir a la cancha y jugar hasta la noche&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Luego, una vez tokenizado con la funci√≥n que desarrollamos en el post anterior, nos queda el siguiente vocabulario: &lt;strong&gt;bolsa&lt;/strong&gt;, &lt;strong&gt;cancha&lt;/strong&gt;, &lt;strong&gt;comer&lt;/strong&gt;, &lt;strong&gt;comprarme&lt;/strong&gt;, &lt;strong&gt;delicioso&lt;/strong&gt;, &lt;strong&gt;ensuciar&lt;/strong&gt;, &lt;strong&gt;f√∫tbol&lt;/strong&gt;, &lt;strong&gt;ganar&lt;/strong&gt;, &lt;strong&gt;jugar&lt;/strong&gt;, &lt;strong&gt;juro&lt;/strong&gt;, &lt;strong&gt;noche&lt;/strong&gt;, &lt;strong&gt;plato&lt;/strong&gt;, &lt;strong&gt;pl√°stico&lt;/strong&gt;, &lt;strong&gt;suadero&lt;/strong&gt;, &lt;strong&gt;taco&lt;/strong&gt; y &lt;strong&gt;vida&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Como referencia, aqu√≠ est√° la funci√≥n tokenizadora que llamamos &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenize_phrase&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_punct&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_stop&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spanish_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;SciKit-Learn&lt;/strong&gt;: Si no est√°s familiarizado con la interfaz de SciKit-Learn te invito a &lt;a href=&quot;https://www.youtube.com/watch?v=XgXPxrEg0rA&quot;&gt;ver mi video sobre el tema&lt;/a&gt;. En resumen, &lt;em&gt;SciKit-Learn&lt;/em&gt; tiene unas clases que se conocen como &lt;em&gt;transformers&lt;/em&gt;, estos, como el nombre nos indica, son usados para transformar datos entre dos dominios. Los transformadores que vamos a usar poseen dos m√©todos que estamos usando: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt;, que nos ayuda preparar nuestro vectorizador, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transform&lt;/code&gt; que nos ayuda a convertir nuestros datos a vectores.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;bolsa-de-palabras-bag-of-words&quot;&gt;Bolsa de palabras (&lt;em&gt;Bag of words&lt;/em&gt;)&lt;/h2&gt;
&lt;p&gt;Una de las primeras ideas que se nos puede venir a la mente es la de generar una peque√±a tabla, en donde cada documento es un rengl√≥n y cada token es una columna:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/004_empty.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CountVectorizer&lt;/code&gt; y &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;analyzer&lt;/code&gt;&lt;/strong&gt;: Al especificar el argumento &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;analyzer&lt;/code&gt; y asignarlo a nuestra funci√≥n para tokenizar (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenize_phrase&lt;/code&gt;) le estamos indicando a nuestro transformador que cuando sea el momento de tokenizar nuestros documentos, use esa funci√≥n en lugar del tokenizador por default que viene en &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;vectores-de-frecuencia&quot;&gt;Vectores de frecuencia&lt;/h3&gt;
&lt;p&gt;Para rellenar la tabla anterior tenemos varias opciones, por ejemplo, podr√≠amos simplemente la cantidad de veces que un determinado token aparece en cada uno de los documentos:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/004_frequency.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Como, en nuestro caso, la palabra ‚Äúsuadero‚Äù aparece tres veces en el primer documento.&lt;/p&gt;

&lt;p&gt;Para lograr esto, vamos a hacer uso de &lt;em&gt;SciKit-Learn&lt;/em&gt;, que si bien no es un framework dedicado a trabajar con texto, tiene bastantes utilidades que nos permiten hacerlo de manera sencilla y reproducible.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;frequency_vectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;frequency_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;frequency_vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frequency_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;one-hot-encoding-bag-of-words&quot;&gt;One-hot-encoding (&lt;em&gt;Bag of words&lt;/em&gt;)&lt;/h3&gt;
&lt;p&gt;Otra opci√≥n que tenemos, si es que solamente queremos saber si una palabra existe en un documento o no, es la de usar la codificaci√≥n &lt;em&gt;one-hot&lt;/em&gt;, que simplemente consiste en colocar verdadero o falso (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt; o &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;) dependiendo de si la palabra existe o no:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/004_onehot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Como, en nuestro caso, a pesar de que la palabra ‚Äúsuadero‚Äù aparece tres veces en el primer documento, solamente hay un uno en la columna correspondiente. Para lograr esto con Python, el siguiente c√≥digo es √∫til:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;one_hot_vectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;one_hot_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;one_hot_vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CountVectorizer&lt;/code&gt; y &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary&lt;/code&gt;&lt;/strong&gt;: A pesar de usar la misma clase &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CountVectorizer&lt;/code&gt;, el especificar el argumento &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary = True&lt;/code&gt; le indica a nuestro &lt;em&gt;transformer&lt;/em&gt; que simplemente queremos saber si un token existe en un documento, no cu√°ntas veces aparece.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;tf-idf&quot;&gt;TF-IDF&lt;/h3&gt;
&lt;p&gt;Tanto el vector de frecuencias como el binario tratan cada documento de manera individual. Lo cual es bueno hasta cierto punto, sin embargo, existe una transformaci√≥n que toma en cuenta la influencia de todo el &lt;em&gt;corpus&lt;/em&gt; en cada documento.&lt;/p&gt;

&lt;p&gt;La idea detr√°s del famoso TF-IDF (Term Frequency ‚Äì Inverse Document Frequency), o por su nombre en espa√±ol: &lt;em&gt;frecuencia de t√©rmino ‚Äì frecuencia inversa de documento&lt;/em&gt;, El valor de cada token aumenta proporcionalmente al n√∫mero de veces que una palabra aparece en el documento, pero es compensada por la frecuencia de la palabra en la colecci√≥n de documentos, lo que permite manejar el hecho de que algunas palabras son generalmente m√°s comunes que otras.&lt;/p&gt;

&lt;h3 id=&quot;detalles-matem√°ticos&quot;&gt;Detalles matem√°ticos‚Ä¶&lt;/h3&gt;

&lt;p&gt;Existen diversas maneras de calcular este n√∫mero &lt;a href=&quot;https://es.wikipedia.org/wiki/Tf-idf#Detalles_matem%C3%A1ticos&quot;&gt;la wikipedia lista varias&lt;/a&gt;, pero esta es una de las m√°s comunes:&lt;/p&gt;

&lt;p&gt;Partiendo de que&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(t\) es el token para el cual estamos calculando,&lt;/li&gt;
  &lt;li&gt;\(d\) es el documento de inter√©s y&lt;/li&gt;
  &lt;li&gt;\(D\) es nuestro corpus&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obtenemos el valor de acuerdo a la siguiente formula&lt;/p&gt;

\[tfidf(t, d, D) = \color{blue}{tf(t, d)} \times \color{red}{idf(t, D)}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(\color{blue}{tf(t, d) = f_{t,d}}\) es la cantidad de veces que el token \(t\) aparece en el documento \(d\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\color{red}{idf(t, D) = \ln{\frac{\vert D \vert + 1}{\vert\{d \in D: t \in d\}\vert + 1} + 1}}\) en donde \(\vert D \vert\) es la cantidad de documentos en nuestro corpus y \(\vert \{d \in D: t \in d\} \vert\) es la cantidad de documentos en la que aparece el token \(t\). Los \(+ 1\) que se encuentran ah√≠ son conocidos como ‚Äúsuavizado‚Äù que nos ayudan a evitar divisiones entre \(0\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Toma por ejemplo, el token &lt;em&gt;suadero&lt;/em&gt; en el primer documento (denotado como \(documentos_1\)):&lt;/p&gt;

\[tfidf(‚Äúsuadero‚Äù, documentos_1, corpus) = \color{blue}{tf(‚Äúsuadero‚Äù, documentos_1)} \times \color{red}{idf(‚Äúsuadero‚Äù, corpus)}\]

\[tfidf(‚Äúsuadero‚Äù, documentos_1, corpus) = \color{blue}{3} \times \color{red}{\ln{\frac{3 + 1}{2 + 1} + 1}}\]

\[tfidf(‚Äúsuadero‚Äù, documentos_1, corpus) = \color{blue}{3} \times \color{red}{\ln{\frac{4}{3}+1}}\]

\[tfidf(‚Äúsuadero‚Äù, documentos_1, corpus) = \color{blue}{3} \times \color{red}{\ln{2.333}}\]

\[tfidf(‚Äúsuadero‚Äù, documentos_1, corpus) = 3.8630\]

&lt;p&gt;Como se puede observar en la siguiente tabla:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/004_tfidf_no_norm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Usualmente los algoritmos lineares de &lt;em&gt;machine learning&lt;/em&gt; otorgan mejores resultados cuando nuestras variables de entrada est√°n normalizadas, as√≠ que se recomienda que nosotros hagamos lo mismo.&lt;/p&gt;

&lt;p&gt;Para nuestra suerte, dentro de &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; tambi√©n existe un &lt;em&gt;transformer&lt;/em&gt; que nos permite convertir nuestros vectores a TF-IDF (incluyendo la parta de la normalizaci√≥n):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tfidf_vectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tfidf_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tfidf_vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidf_vectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Que nos dar√° como resultado una matriz m√°s o menos as√≠:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/004_tfidf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sobre-las-representaciones-vectoriales&quot;&gt;Sobre las representaciones vectoriales‚Ä¶&lt;/h2&gt;

&lt;h3 id=&quot;representaciones-dispersas&quot;&gt;Representaciones dispersas&lt;/h3&gt;
&lt;p&gt;Si de algo nos dimos cuenta con las matrices anteriores, es que en muchas ocasiones, tenemos m√°s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; (valores en blanco) que valores reales. En otras palabras, nuestros documentos-vectores son dispersos, o en ingles &lt;em&gt;sparse vectors&lt;/em&gt;. Esto podr√≠a llegar a ser un problema cuando tenemos un vocabulario de tama√±o considerable.&lt;/p&gt;

&lt;h3 id=&quot;no-reflejan-el-orden-de-los-t√©rminos&quot;&gt;No reflejan el orden de los t√©rminos&lt;/h3&gt;
&lt;p&gt;Otra cosa a considerar es que con solo ver la representaci√≥n vectorizada de determinado documento, no podemos reconstruir el documento original, de entrada por todo el proceso de tokenizaci√≥n, pero tambi√©n porque &lt;strong&gt;ninguna de las t√©cnicas que vimos, preserva el orden original de los tokens en el documento&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;existen-todav√≠a-m√°s-opciones&quot;&gt;Existen todav√≠a m√°s opciones&lt;/h3&gt;
&lt;p&gt;Si bien estas representaciones son buenas u √∫tiles, hay casos en los que necesitamos obtener m√°s detalle de nuestros documentos, para esos casos podemos usar t√©cnicas como &lt;a href=&quot;http://jalammar.github.io/illustrated-word2vec/&quot;&gt;Word2vec&lt;/a&gt; o &lt;a href=&quot;https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e&quot;&gt;Doc2vec&lt;/a&gt;, de las cuales espero escribir en un post futuro.&lt;/p&gt;

&lt;h3 id=&quot;guarda-tus-vectorizadores&quot;&gt;Guarda tus vectorizadores&lt;/h3&gt;
&lt;p&gt;Sin importar qu√© m√©todo de transformaci√≥n usaste, siempre debes usar el mismo para transformaciones subsecuentes. Por ejemplo, si usaste &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt; con un transformador para los datos de entrenamiento de un algoritmo de clasificaci√≥n, debes usar el mismo transformador (ya entrenado) para obtener los vectores al momento de realizar predicciones en nuevos datos.&lt;/p&gt;

&lt;p&gt;Mientras tanto, te invito a que me en Twitter en &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;, por all√° podemos conversar sobre el tema.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Antonio Feregrino Bola√±os</name>
        
        
      </author>

      

      
        <category term="python" />
      
        <category term="skelarn" />
      
        <category term="texto" />
      

      
        <summary type="html">Tener nuestro texto convertido en tokens es un paso importante en el uso de texto para aplicaciones de machine learning, existe una transformaci√≥n que debemos realizar para facilitarle la tarea a nuestros modelos. Esta transformaci√≥n es conocida como vectorizaci√≥n.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducci√≥n al an√°lisis de texto</title>
      <link href="https://old.tacosdedatos.com/analisis-texto" rel="alternate" type="text/html" title="Introducci√≥n al an√°lisis de texto" />
      <published>2020-08-16T10:00:00+00:00</published>
      <updated>2020-08-16T10:00:00+00:00</updated>
      <id>https://old.tacosdedatos.com/analisis-texto</id>
      <content type="html" xml:base="https://old.tacosdedatos.com/analisis-texto">&lt;p&gt;¬øAlguna vez has querido analizar texto? en este post voy a tratar de explicar cu√°les son los primeros pasos recomendados para comenzar cualquier proyecto que involucre texto y ciencia de datos.&lt;/p&gt;

&lt;h2 id=&quot;texto-como-datos&quot;&gt;Texto como datos&lt;/h2&gt;
&lt;p&gt;Seguramente ya habr√°s escuchado algunas veces, o tu mismo has dicho ‚ÄúHey Google‚Äù o ‚ÄúHey Siri‚Äù, o tal vez te le has echado un ojo a tu filtro de spam‚Ä¶ pero ¬øte has preguntado c√≥mo es que funcionan estos sistemas? como ya te imaginar√°s, la mayor√≠a son aplicaciones de aprendizaje autom√°tico (o &lt;em&gt;machine learning&lt;/em&gt;) que son posibles gracias a muchos de los algoritmos tradicionales del aprendizaje m√°quina.&lt;/p&gt;

&lt;p&gt;Sin embargo, no podemos nosotros simplemente agarrar un mont√≥n de texto y d√°rselo a un algoritmo y esperar a que haga su magia‚Ä¶ antes de todo esto existe un proceso que les voy a describir a continuaci√≥n.&lt;/p&gt;

&lt;h2 id=&quot;spacy&quot;&gt;&lt;em&gt;spaCy&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Aqu√≠ es donde entra &lt;em&gt;spaCy&lt;/em&gt;, que es un paquete de Python que podr√≠amos comparar con una navaja suiza para el procesamiento de texto. Esta es una herramienta muy poderosa, y aqu√≠ solamente vamos a tocar apenas la superficie de lo que ofrece. Para comenzar, hay que instalarlo:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;spacy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instalar &lt;em&gt;spaCy&lt;/em&gt; es solo la primera parte del rompecabezas, puesto que su correcto funcionamiento depende de usar el modelo adecuado para la tarea (y el idioma) que vamos a realizar, aqu√≠ puedes encontrar una &lt;a href=&quot;https://spacy.io/models&quot;&gt;descripci√≥n de los modelos&lt;/a&gt;, pero para este post, podemos usar el m√°s simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;es_core_news_sm&lt;/code&gt;, se descarga con estos comandos en la consola:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; spacy download es_core_news_sm 
python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; spacy &lt;span class=&quot;nb&quot;&gt;link &lt;/span&gt;es_core_news_sm es
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para acceder dentro de Python a todas las bondades de nuestra nueva herramienta, es necesario cargar el modelo, es convenci√≥n cargar el modelo en una variable llamada &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nlp&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;es&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Y listo, por el momento podemos seguir.&lt;/p&gt;

&lt;h2 id=&quot;tokenizaci√≥n&quot;&gt;Tokenizaci√≥n&lt;/h2&gt;
&lt;p&gt;El primer paso es segmentar nuestro texto en tokens. Un token es un conjunto de caracteres que representan texto. Tambi√©n podemos decir que el token es la √∫nidad an√°lisis de texto, as√≠ como un n√∫mero es la unidad del an√°lisis matem√°tico. Es f√°cil para nosotros pensar que un token es igual a una palabra, sin embargo esto no es correcto, puesto que la ‚Äúpalabra‚Äù es un elemento del lenguaje que posee significado por s√≠ misma, mientras que el token se supone es un elemento abstracto. Dependiendo de la tarea que estemos afrontando, el token puede ser alguna de las siguientes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Una sola palabra, como: ‚Äúj√≥venes‚Äù, ‚Äúnivel‚Äù o ‚Äúsuperior‚Äù,&lt;/li&gt;
  &lt;li&gt;Un n√∫mero, como: ‚Äú1‚Äù, ‚Äú0‚Äù, o ‚Äú10‚Äù,&lt;/li&gt;
  &lt;li&gt;Un solo caracter, como: ‚Äúj‚Äù, ‚Äú√≥‚Äù o ‚Äúv‚Äù,&lt;/li&gt;
  &lt;li&gt;Un s√≠mbolo, como ‚Äú¬ø‚Äù, ‚Äú?‚Äù o ‚Äú#‚Äù,&lt;/li&gt;
  &lt;li&gt;Un conjunto de caracteres, como ‚Äúnivel superior‚Äù o ‚Äúescuela t√©cnica‚Äù&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La forma de elegir los tokens en nuestro texto va a depender much√≠simo del problema que estemos afrontando, habr√° ocasiones en las que una simple tokenizaci√≥n, como la de dividir nuestro texto por los espacios, bastar√°:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Seguramente ya habr√°s escuchado algunas veces, o tu mismo has dicho &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Hey Google&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; o &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Hey Siri&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, o tal vez le has echado un ojo a tu filtro de spam...&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;simple_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simple_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['Seguramente', 'ya', 'habr√°s', 'escuchado', 'algunas', 'veces,', 'o', 'tu', 'mismo', 'has', 'dicho', '&quot;Hey', 'Google&quot;', 'o', '&quot;Hey', 'Siri&quot;,', 'o', 'tal', 'vez', 'le', 'has', 'echado', 'un', 'ojo', 'a', 'tu', 'filtro', 'de', 'spam...']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pero habr√° otras ocasiones en las que tengamos que echar mano de otras formas de tokenizar el texto, por ejemplo en el paquete &lt;em&gt;spaCy&lt;/em&gt; podemos echar mano de algunas herramientas.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Seguramente, ya, habr√°s, escuchado, algunas, veces, ,, o, tu, mismo, has, dicho, &quot;, Hey, Google, &quot;, o, &quot;, Hey, Siri, &quot;, ,, o, tal, vez, le, has, echado, un, ojo, a, tu, filtro, de, spam, ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La gran diferencia es que en la segunda tokenizaci√≥n los s√≠mbolos de puntuaci√≥n est√°n separadas de las palabras en el texto.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: Cuando llamamos &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nlp(&quot;alg√∫n texto&quot;)&lt;/code&gt; obtenemos como retorno un valor del tipo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Document&lt;/code&gt;, que, a su vez est√° compuesto de valores del tipo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt;, es por eso que podemos iterar nuestro documento con un ciclo &lt;em&gt;for&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reducci√≥n-de-tokens&quot;&gt;Reducci√≥n de tokens&lt;/h2&gt;

&lt;h3 id=&quot;min√∫sculas-may√∫sculas&quot;&gt;¬øMin√∫sculas? ¬ømay√∫sculas?&lt;/h3&gt;
&lt;p&gt;Hasta este punto todo bien con nuestros tokens, pero piensa en una oraci√≥n como:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;El ciclo escolar comienza en las escuelas. Escuelas de todo el pa√≠s comenzar√°n clases este lunes.&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Los tokens √∫nicos de esta oraci√≥n son:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'clases', 'El', 'las', 'en', 'todo', 'comenzar√°n', 'este', 'ciclo', 'escolar', 'comienza', 'escuelas', '.', 'el', 'lunes', 'Escuelas', 'pa√≠s', 'de'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Si te das cuenta, ‚Äú&lt;em&gt;Escuelas&lt;/em&gt;‚Äù y ‚Äú&lt;em&gt;escuelas&lt;/em&gt;‚Äù son dos tokens distintos, (recuerda, los tokens no son palabras); sin embargo, para muchas aplicaciones, estos dos tokens pueden, simple y sencillamente ser tratados como el mismo, simplemente con transformar todos nuestros tokens a min√∫sculas. Usamos la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lower_&lt;/code&gt; de &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt; para acceder a la versi√≥n en min√∫scula de la palabra:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'pa√≠s', 'lunes', 'comenzar√°n', 'en', '.', 'el', 'comienza', 'de', 'ciclo', 'las', 'escolar', 'clases', 'este', 'todo', 'escuelas'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: como ya sabemos, en nuestro ejemplo de c√≥digo anterior, cuando iteramos sobre &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;oracion_parsed&lt;/code&gt;, obtenemos uno a uno los tokens que forman nuestro documento original. Cada uno de estos elementos de la clase &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt; posee &lt;a href=&quot;https://spacy.io/api/token&quot;&gt;muchas propiedades&lt;/a&gt;, de entre ellas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lower_&lt;/code&gt; es una.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;stopwords-signos-de-puntuaci√≥n&quot;&gt;¬ø&lt;em&gt;Stopwords&lt;/em&gt;? ¬øSignos de puntuaci√≥n?&lt;/h3&gt;

&lt;p&gt;Como resultado de la tokenizaci√≥n, en el ejemplo anterior vemos que existen muchos &lt;em&gt;tokens&lt;/em&gt; que, dependiendo del an√°lisis que vayamos a realizar, podemos considerar ‚Äúirrelevantes‚Äù para nuestro an√°lisis, por ejemplo, las palabras ‚Äúen‚Äù, ‚Äúeste‚Äù, ‚Äúel‚Äù y ‚Äúlas‚Äù ya que son muy comunes en el espa√±ol y est√°n destinadas a aparecer en todas las oraciones, sin importar el tema al que estas hagan referencia. Estas palabras en ingl√©s son conocidas como ‚Äú&lt;em&gt;stopwords&lt;/em&gt;‚Äù. Para encontrarlas con &lt;em&gt;spaCy&lt;/em&gt; podemos usar la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_stop&lt;/code&gt; de cada token:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_stop&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['el: True', 'ciclo: False', 'escolar: False', 'comienza: False', 'en: True', 'las: True', 'escuelas: False', '.: False', 'escuelas: False', 'de: True', 'todo: True', 'el: True', 'pa√≠s: False', 'comenzar√°n: False', 'clases: False', 'este: True', 'lunes: False', '.: False']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Al igual que las stopwords puede ser que los s√≠mbolos carezcan de relevancia en nuestro an√°lisis, nuevamente, por ser considerados como una ocurrencia com√∫n en el espa√±ol. Para encontrarlos, podemos hacer uso de la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_punct&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_punct&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['el: False', 'ciclo: False', 'escolar: False', 'comienza: False', 'en: False', 'las: False', 'escuelas: False', '.: True', 'escuelas: False', 'de: False', 'todo: False', 'el: False', 'pa√≠s: False', 'comenzar√°n: False', 'clases: False', 'este: False', 'lunes: False', '.: True']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;lematizaci√≥n&quot;&gt;Lematizaci√≥n&lt;/h3&gt;

&lt;p&gt;Otro procedimiento que podemos usar para reducir la cantidad de tokens √∫nicos es el proceso de lematizaci√≥n, que es un proceso ling√º√≠stico que consiste en, dada una forma flexionada, hallar el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra‚Ä¶ pero creo que es mejor con algunos ejemplos:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comienza -&amp;gt; comenzar&lt;/li&gt;
  &lt;li&gt;comenzar√°n -&amp;gt; comenzar&lt;/li&gt;
  &lt;li&gt;clases -&amp;gt; clase&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'pa√≠s', 'lunes', 'en', '.', 'el', 'los', 'de', 'escolar', 'clase', 'este', 'ciclar', 'comenzar', 'escuela', 'todo', 'escuelas'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;stemming---acortamiento-&quot;&gt;Stemming - Acortamiento (?)&lt;/h3&gt;

&lt;p&gt;Existe otra forma de reducir el n√∫mero de tokens. Y es conocida como &lt;em&gt;stemming&lt;/em&gt; en ingl√©s, en espa√±ol yo lo traducir√≠a como ‚Äúacortamiento‚Äù o ‚Äúpoda (de podar el c√©sped)‚Äù. Este proceso consiste en simple y llanamente recortar las palabras para reduciras a una base com√∫n:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comienza -&amp;gt; comienz&lt;/li&gt;
  &lt;li&gt;comenzar√°n -&amp;gt; comenz&lt;/li&gt;
  &lt;li&gt;clases -&amp;gt; clas&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem.snowball&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SnowballStemmer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SnowballStemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spanish'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'escol', 'en', 'comienz', 'el', '.', 'de', 'comenz', 'las', 'lun', 'pais', 'este', 'clas', 'tod', 'cicl', 'escuel'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: por incre√≠ble que parezca, esta poderosa librer√≠a no cuenta con la opci√≥n de ejecutar &lt;em&gt;stemming&lt;/em&gt; por default, las &lt;a href=&quot;https://github.com/explosion/spaCy/issues/327#issuecomment-208658745&quot;&gt;razones son varias&lt;/a&gt; y tratar√© de hablar sobre ellas m√°s adelante, pero por ahora, si quieres realizar &lt;em&gt;stemming&lt;/em&gt;, el paquete NLTK es tu aliado.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;preguntas-m√°s-comunes&quot;&gt;Preguntas m√°s comunes&lt;/h3&gt;

&lt;h4 id=&quot;lematizaci√≥n-o-stemming&quot;&gt;¬øLematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt;?&lt;/h4&gt;

&lt;p&gt;Estas dos t√©cnicas son consideradas mutuamente exclusivas, puesto que o aplicas una o aplicas la otra, nunca las dos. Pero, ¬øcu√°l es la m√°s recomendada?&lt;/p&gt;

&lt;p&gt;En general siempre se prefiere la lematizaci√≥n, puesto que es un buen compromiso entre reducir la cantidad de tokens y preservar un poco m√°s la composici√≥n original de estos. El &lt;em&gt;stemming&lt;/em&gt; al ser m√°s agresivo tiende a conllevar una p√©rdida de informaci√≥n m√°s grande.&lt;/p&gt;

&lt;h4 id=&quot;cu√°l-es-el-orden-en-que-se-aplican-los-pasos&quot;&gt;¬øCu√°l es el orden en que se aplican los pasos?&lt;/h4&gt;
&lt;p&gt;Es com√∫n que despu√©s de tokenizar el texto, los pasos se apliquen en el orden presentado:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Conversi√≥n a min√∫sculas&lt;/li&gt;
  &lt;li&gt;Eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt; y s√≠mbolos de puntuaci√≥n&lt;/li&gt;
  &lt;li&gt;Lematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Siempre es muy importante realizar el proceso de eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt; antes de transformar los tokens, puesto que de otro modo puede que el proceso de transformaci√≥n convierta ‚Äúaccidentalmente‚Äù nuestros tokens √∫tiles en &lt;em&gt;stopwords&lt;/em&gt; y esto nos lleve a perder informaci√≥n valiosa de nuestro dataset.&lt;/p&gt;

&lt;h4 id=&quot;por-qu√©-querr√≠amos-reducir-la-cantidad-de-informaci√≥n-mediante-estas-transformaciones&quot;&gt;¬øPor qu√© querr√≠amos ‚Äúreducir‚Äù la cantidad de informaci√≥n mediante estas transformaciones?&lt;/h4&gt;
&lt;p&gt;La idea detr√°s de la eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt;, s√≠mbolos, la lematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt; radica en reducir la cantidad de elementos √∫nicos en nuestro dataset, con el objetivo de incrementar el desempe√±o de nuestro algoritmo de dos maneras:&lt;/p&gt;

&lt;p&gt;El eliminar las &lt;em&gt;stopwords&lt;/em&gt; nos ayuda a elminiar palabras comunes que tienen poco valor discriminativo entre textos. As√≠ mismo, para muchos problemas no necesitamos conocer el tiempo en el que un verbo estaba escrito, o si la palabra era ‚Äúcorrupto‚Äù o ‚Äúcorrupci√≥n‚Äù; con las formas base el algoritmo puede ‚Äúaprender‚Äù una idea general.&lt;/p&gt;

&lt;p&gt;Esta misma idea se puede aplicar para tokens muy raros dentro de nuestro texto‚Ä¶ podr√≠amos eliminar tokens que no aparezcan m√°s de $X$ cantidad de veces, bajo la sospecha de que tal vez fueron errores ortogr√°ficos o palabras sin importancia.&lt;/p&gt;

&lt;p&gt;Si tienes m√°s dudas o comentarios, no olvides hac√©rmelas llegar a trav√©s de los comentarios en esta p√°gina, o, para mayor seguridad, a mi cuenta de Twitter &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;y-las-visualizaciones&quot;&gt;¬øY las visualizaciones?&lt;/h3&gt;
&lt;p&gt;Para no fallarle al p√∫blico de Tacos de Datos, vamos a hacer una sencill√≠sima visualizaci√≥n con las palabras que m√°s frecuente usa el presidente mexicano en sus conferencias (ojo, ac√° no vamos a hablar de pol√≠tica). Y de paso aprovecho para presentarles un dataset que estoy manteniendo en Kaggle, en el cual coloco m√°s o menos semanalmente todas las versiones estenogr√°ficas de las conferencias que publica la presidencia de M√©xico. Puedes acceder al dataset aqu√≠ &lt;a href=&quot;https://www.kaggle.com/ioexception/mananeras&quot;&gt;Conferencias Ma√±aneras&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;El primer paso es crear una funci√≥n que nos ayude a tokenizar texto, esta es una funci√≥n que debemos conservar, ya que si en el futuro queremos repetir nuestros experimentos, debemos usar la misma forma de tokenizar para que exista consistencia en nuestros resultados.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.corpus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spanish_stopwords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spanish'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_punct&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_stop&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spanish_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Despu√©s, con todas nuestras frases cargadas en el arreglo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lopez_obrador&lt;/code&gt; rellenamos un &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Counter&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amlo_phrase&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lopez_obrador&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amlo_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Una vez que tenemos el objeto &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word_counter&lt;/code&gt; con los valores, podemos graficarlo:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Las 20 m√°s frecuentemente usadas&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;por L√≥pez Obrador en sus discursos&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/003_amlo_words.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Algunas cosas esperadas y otras no tanto: habr√° que ver por qu√© se la pasa diciendo ‚Äúmil‚Äù y ‚Äúcasar‚Äù‚Ä¶ habr√° que ver tambi√©n si estas palabras no son sino las formas base de otras que tal vez hagan m√°s sentido (puede que al lematizar nuestros tokens estemos perdiendo esta informaci√≥n). Como dije, esto es solo el comienzo del an√°lisis de texto, ¬°a√∫n falta mucho por aprender!&lt;/p&gt;

&lt;p&gt;Si tienes alguna duda con lo presentado en este post, repito: preg√∫ntame por Twitter en &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Antonio Feregrino Bola√±os</name>
        
        
      </author>

      

      
        <category term="python" />
      
        <category term="spacy" />
      
        <category term="texto" />
      

      
        <summary type="html">¬øAlguna vez has querido analizar texto? en este post voy a tratar de explicar cu√°les son los primeros pasos recomendados para comenzar cualquier proyecto que involucre texto y ciencia de datos.</summary>
      

      
      
    </entry>
  
</feed>
