<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator>
  <link href="https://old.tacosdedatos.com/tag/spacy/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://old.tacosdedatos.com/" rel="alternate" type="text/html" />
  <updated>2021-08-01T20:46:16+00:00</updated>
  <id>https://old.tacosdedatos.com/tag/spacy/feed.xml</id>

  
  
  

  
    <title type="html">üåÆ tacos de datos | Aprende visualizaci√≥n de datos en espa√±ol. | </title>
  

  
    <subtitle>Tu sitio para aprender de visualizaci√≥n y ciencia de datos en espa√±ol. Consejos, recursos y mejores pr√°cticas para tus proyectos de tecnolog√≠a, periodismo de datos y an√°lisis estad√≠sticos.</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Introducci√≥n al an√°lisis de texto</title>
      <link href="https://old.tacosdedatos.com/analisis-texto" rel="alternate" type="text/html" title="Introducci√≥n al an√°lisis de texto" />
      <published>2020-08-16T10:00:00+00:00</published>
      <updated>2020-08-16T10:00:00+00:00</updated>
      <id>https://old.tacosdedatos.com/analisis-texto</id>
      <content type="html" xml:base="https://old.tacosdedatos.com/analisis-texto">&lt;p&gt;¬øAlguna vez has querido analizar texto? en este post voy a tratar de explicar cu√°les son los primeros pasos recomendados para comenzar cualquier proyecto que involucre texto y ciencia de datos.&lt;/p&gt;

&lt;h2 id=&quot;texto-como-datos&quot;&gt;Texto como datos&lt;/h2&gt;
&lt;p&gt;Seguramente ya habr√°s escuchado algunas veces, o tu mismo has dicho ‚ÄúHey Google‚Äù o ‚ÄúHey Siri‚Äù, o tal vez te le has echado un ojo a tu filtro de spam‚Ä¶ pero ¬øte has preguntado c√≥mo es que funcionan estos sistemas? como ya te imaginar√°s, la mayor√≠a son aplicaciones de aprendizaje autom√°tico (o &lt;em&gt;machine learning&lt;/em&gt;) que son posibles gracias a muchos de los algoritmos tradicionales del aprendizaje m√°quina.&lt;/p&gt;

&lt;p&gt;Sin embargo, no podemos nosotros simplemente agarrar un mont√≥n de texto y d√°rselo a un algoritmo y esperar a que haga su magia‚Ä¶ antes de todo esto existe un proceso que les voy a describir a continuaci√≥n.&lt;/p&gt;

&lt;h2 id=&quot;spacy&quot;&gt;&lt;em&gt;spaCy&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Aqu√≠ es donde entra &lt;em&gt;spaCy&lt;/em&gt;, que es un paquete de Python que podr√≠amos comparar con una navaja suiza para el procesamiento de texto. Esta es una herramienta muy poderosa, y aqu√≠ solamente vamos a tocar apenas la superficie de lo que ofrece. Para comenzar, hay que instalarlo:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;spacy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instalar &lt;em&gt;spaCy&lt;/em&gt; es solo la primera parte del rompecabezas, puesto que su correcto funcionamiento depende de usar el modelo adecuado para la tarea (y el idioma) que vamos a realizar, aqu√≠ puedes encontrar una &lt;a href=&quot;https://spacy.io/models&quot;&gt;descripci√≥n de los modelos&lt;/a&gt;, pero para este post, podemos usar el m√°s simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;es_core_news_sm&lt;/code&gt;, se descarga con estos comandos en la consola:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; spacy download es_core_news_sm 
python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; spacy &lt;span class=&quot;nb&quot;&gt;link &lt;/span&gt;es_core_news_sm es
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para acceder dentro de Python a todas las bondades de nuestra nueva herramienta, es necesario cargar el modelo, es convenci√≥n cargar el modelo en una variable llamada &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nlp&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;es&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Y listo, por el momento podemos seguir.&lt;/p&gt;

&lt;h2 id=&quot;tokenizaci√≥n&quot;&gt;Tokenizaci√≥n&lt;/h2&gt;
&lt;p&gt;El primer paso es segmentar nuestro texto en tokens. Un token es un conjunto de caracteres que representan texto. Tambi√©n podemos decir que el token es la √∫nidad an√°lisis de texto, as√≠ como un n√∫mero es la unidad del an√°lisis matem√°tico. Es f√°cil para nosotros pensar que un token es igual a una palabra, sin embargo esto no es correcto, puesto que la ‚Äúpalabra‚Äù es un elemento del lenguaje que posee significado por s√≠ misma, mientras que el token se supone es un elemento abstracto. Dependiendo de la tarea que estemos afrontando, el token puede ser alguna de las siguientes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Una sola palabra, como: ‚Äúj√≥venes‚Äù, ‚Äúnivel‚Äù o ‚Äúsuperior‚Äù,&lt;/li&gt;
  &lt;li&gt;Un n√∫mero, como: ‚Äú1‚Äù, ‚Äú0‚Äù, o ‚Äú10‚Äù,&lt;/li&gt;
  &lt;li&gt;Un solo caracter, como: ‚Äúj‚Äù, ‚Äú√≥‚Äù o ‚Äúv‚Äù,&lt;/li&gt;
  &lt;li&gt;Un s√≠mbolo, como ‚Äú¬ø‚Äù, ‚Äú?‚Äù o ‚Äú#‚Äù,&lt;/li&gt;
  &lt;li&gt;Un conjunto de caracteres, como ‚Äúnivel superior‚Äù o ‚Äúescuela t√©cnica‚Äù&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La forma de elegir los tokens en nuestro texto va a depender much√≠simo del problema que estemos afrontando, habr√° ocasiones en las que una simple tokenizaci√≥n, como la de dividir nuestro texto por los espacios, bastar√°:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Seguramente ya habr√°s escuchado algunas veces, o tu mismo has dicho &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Hey Google&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; o &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Hey Siri&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, o tal vez le has echado un ojo a tu filtro de spam...&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;simple_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simple_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['Seguramente', 'ya', 'habr√°s', 'escuchado', 'algunas', 'veces,', 'o', 'tu', 'mismo', 'has', 'dicho', '&quot;Hey', 'Google&quot;', 'o', '&quot;Hey', 'Siri&quot;,', 'o', 'tal', 'vez', 'le', 'has', 'echado', 'un', 'ojo', 'a', 'tu', 'filtro', 'de', 'spam...']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pero habr√° otras ocasiones en las que tengamos que echar mano de otras formas de tokenizar el texto, por ejemplo en el paquete &lt;em&gt;spaCy&lt;/em&gt; podemos echar mano de algunas herramientas.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Seguramente, ya, habr√°s, escuchado, algunas, veces, ,, o, tu, mismo, has, dicho, &quot;, Hey, Google, &quot;, o, &quot;, Hey, Siri, &quot;, ,, o, tal, vez, le, has, echado, un, ojo, a, tu, filtro, de, spam, ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La gran diferencia es que en la segunda tokenizaci√≥n los s√≠mbolos de puntuaci√≥n est√°n separadas de las palabras en el texto.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: Cuando llamamos &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nlp(&quot;alg√∫n texto&quot;)&lt;/code&gt; obtenemos como retorno un valor del tipo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Document&lt;/code&gt;, que, a su vez est√° compuesto de valores del tipo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt;, es por eso que podemos iterar nuestro documento con un ciclo &lt;em&gt;for&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reducci√≥n-de-tokens&quot;&gt;Reducci√≥n de tokens&lt;/h2&gt;

&lt;h3 id=&quot;min√∫sculas-may√∫sculas&quot;&gt;¬øMin√∫sculas? ¬ømay√∫sculas?&lt;/h3&gt;
&lt;p&gt;Hasta este punto todo bien con nuestros tokens, pero piensa en una oraci√≥n como:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;El ciclo escolar comienza en las escuelas. Escuelas de todo el pa√≠s comenzar√°n clases este lunes.&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Los tokens √∫nicos de esta oraci√≥n son:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'clases', 'El', 'las', 'en', 'todo', 'comenzar√°n', 'este', 'ciclo', 'escolar', 'comienza', 'escuelas', '.', 'el', 'lunes', 'Escuelas', 'pa√≠s', 'de'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Si te das cuenta, ‚Äú&lt;em&gt;Escuelas&lt;/em&gt;‚Äù y ‚Äú&lt;em&gt;escuelas&lt;/em&gt;‚Äù son dos tokens distintos, (recuerda, los tokens no son palabras); sin embargo, para muchas aplicaciones, estos dos tokens pueden, simple y sencillamente ser tratados como el mismo, simplemente con transformar todos nuestros tokens a min√∫sculas. Usamos la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lower_&lt;/code&gt; de &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt; para acceder a la versi√≥n en min√∫scula de la palabra:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'pa√≠s', 'lunes', 'comenzar√°n', 'en', '.', 'el', 'comienza', 'de', 'ciclo', 'las', 'escolar', 'clases', 'este', 'todo', 'escuelas'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: como ya sabemos, en nuestro ejemplo de c√≥digo anterior, cuando iteramos sobre &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;oracion_parsed&lt;/code&gt;, obtenemos uno a uno los tokens que forman nuestro documento original. Cada uno de estos elementos de la clase &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt; posee &lt;a href=&quot;https://spacy.io/api/token&quot;&gt;muchas propiedades&lt;/a&gt;, de entre ellas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lower_&lt;/code&gt; es una.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;stopwords-signos-de-puntuaci√≥n&quot;&gt;¬ø&lt;em&gt;Stopwords&lt;/em&gt;? ¬øSignos de puntuaci√≥n?&lt;/h3&gt;

&lt;p&gt;Como resultado de la tokenizaci√≥n, en el ejemplo anterior vemos que existen muchos &lt;em&gt;tokens&lt;/em&gt; que, dependiendo del an√°lisis que vayamos a realizar, podemos considerar ‚Äúirrelevantes‚Äù para nuestro an√°lisis, por ejemplo, las palabras ‚Äúen‚Äù, ‚Äúeste‚Äù, ‚Äúel‚Äù y ‚Äúlas‚Äù ya que son muy comunes en el espa√±ol y est√°n destinadas a aparecer en todas las oraciones, sin importar el tema al que estas hagan referencia. Estas palabras en ingl√©s son conocidas como ‚Äú&lt;em&gt;stopwords&lt;/em&gt;‚Äù. Para encontrarlas con &lt;em&gt;spaCy&lt;/em&gt; podemos usar la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_stop&lt;/code&gt; de cada token:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_stop&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['el: True', 'ciclo: False', 'escolar: False', 'comienza: False', 'en: True', 'las: True', 'escuelas: False', '.: False', 'escuelas: False', 'de: True', 'todo: True', 'el: True', 'pa√≠s: False', 'comenzar√°n: False', 'clases: False', 'este: True', 'lunes: False', '.: False']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Al igual que las stopwords puede ser que los s√≠mbolos carezcan de relevancia en nuestro an√°lisis, nuevamente, por ser considerados como una ocurrencia com√∫n en el espa√±ol. Para encontrarlos, podemos hacer uso de la propiedad &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_punct&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_punct&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion_parsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['el: False', 'ciclo: False', 'escolar: False', 'comienza: False', 'en: False', 'las: False', 'escuelas: False', '.: True', 'escuelas: False', 'de: False', 'todo: False', 'el: False', 'pa√≠s: False', 'comenzar√°n: False', 'clases: False', 'este: False', 'lunes: False', '.: True']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;lematizaci√≥n&quot;&gt;Lematizaci√≥n&lt;/h3&gt;

&lt;p&gt;Otro procedimiento que podemos usar para reducir la cantidad de tokens √∫nicos es el proceso de lematizaci√≥n, que es un proceso ling√º√≠stico que consiste en, dada una forma flexionada, hallar el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra‚Ä¶ pero creo que es mejor con algunos ejemplos:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comienza -&amp;gt; comenzar&lt;/li&gt;
  &lt;li&gt;comenzar√°n -&amp;gt; comenzar&lt;/li&gt;
  &lt;li&gt;clases -&amp;gt; clase&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'pa√≠s', 'lunes', 'en', '.', 'el', 'los', 'de', 'escolar', 'clase', 'este', 'ciclar', 'comenzar', 'escuela', 'todo', 'escuelas'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;stemming---acortamiento-&quot;&gt;Stemming - Acortamiento (?)&lt;/h3&gt;

&lt;p&gt;Existe otra forma de reducir el n√∫mero de tokens. Y es conocida como &lt;em&gt;stemming&lt;/em&gt; en ingl√©s, en espa√±ol yo lo traducir√≠a como ‚Äúacortamiento‚Äù o ‚Äúpoda (de podar el c√©sped)‚Äù. Este proceso consiste en simple y llanamente recortar las palabras para reduciras a una base com√∫n:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comienza -&amp;gt; comienz&lt;/li&gt;
  &lt;li&gt;comenzar√°n -&amp;gt; comenz&lt;/li&gt;
  &lt;li&gt;clases -&amp;gt; clas&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem.snowball&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SnowballStemmer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SnowballStemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spanish'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oracion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'escol', 'en', 'comienz', 'el', '.', 'de', 'comenz', 'las', 'lun', 'pais', 'este', 'clas', 'tod', 'cicl', 'escuel'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;spaCy&lt;/em&gt;: por incre√≠ble que parezca, esta poderosa librer√≠a no cuenta con la opci√≥n de ejecutar &lt;em&gt;stemming&lt;/em&gt; por default, las &lt;a href=&quot;https://github.com/explosion/spaCy/issues/327#issuecomment-208658745&quot;&gt;razones son varias&lt;/a&gt; y tratar√© de hablar sobre ellas m√°s adelante, pero por ahora, si quieres realizar &lt;em&gt;stemming&lt;/em&gt;, el paquete NLTK es tu aliado.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;preguntas-m√°s-comunes&quot;&gt;Preguntas m√°s comunes&lt;/h3&gt;

&lt;h4 id=&quot;lematizaci√≥n-o-stemming&quot;&gt;¬øLematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt;?&lt;/h4&gt;

&lt;p&gt;Estas dos t√©cnicas son consideradas mutuamente exclusivas, puesto que o aplicas una o aplicas la otra, nunca las dos. Pero, ¬øcu√°l es la m√°s recomendada?&lt;/p&gt;

&lt;p&gt;En general siempre se prefiere la lematizaci√≥n, puesto que es un buen compromiso entre reducir la cantidad de tokens y preservar un poco m√°s la composici√≥n original de estos. El &lt;em&gt;stemming&lt;/em&gt; al ser m√°s agresivo tiende a conllevar una p√©rdida de informaci√≥n m√°s grande.&lt;/p&gt;

&lt;h4 id=&quot;cu√°l-es-el-orden-en-que-se-aplican-los-pasos&quot;&gt;¬øCu√°l es el orden en que se aplican los pasos?&lt;/h4&gt;
&lt;p&gt;Es com√∫n que despu√©s de tokenizar el texto, los pasos se apliquen en el orden presentado:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Conversi√≥n a min√∫sculas&lt;/li&gt;
  &lt;li&gt;Eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt; y s√≠mbolos de puntuaci√≥n&lt;/li&gt;
  &lt;li&gt;Lematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Siempre es muy importante realizar el proceso de eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt; antes de transformar los tokens, puesto que de otro modo puede que el proceso de transformaci√≥n convierta ‚Äúaccidentalmente‚Äù nuestros tokens √∫tiles en &lt;em&gt;stopwords&lt;/em&gt; y esto nos lleve a perder informaci√≥n valiosa de nuestro dataset.&lt;/p&gt;

&lt;h4 id=&quot;por-qu√©-querr√≠amos-reducir-la-cantidad-de-informaci√≥n-mediante-estas-transformaciones&quot;&gt;¬øPor qu√© querr√≠amos ‚Äúreducir‚Äù la cantidad de informaci√≥n mediante estas transformaciones?&lt;/h4&gt;
&lt;p&gt;La idea detr√°s de la eliminaci√≥n de &lt;em&gt;stopwords&lt;/em&gt;, s√≠mbolos, la lematizaci√≥n o &lt;em&gt;stemming&lt;/em&gt; radica en reducir la cantidad de elementos √∫nicos en nuestro dataset, con el objetivo de incrementar el desempe√±o de nuestro algoritmo de dos maneras:&lt;/p&gt;

&lt;p&gt;El eliminar las &lt;em&gt;stopwords&lt;/em&gt; nos ayuda a elminiar palabras comunes que tienen poco valor discriminativo entre textos. As√≠ mismo, para muchos problemas no necesitamos conocer el tiempo en el que un verbo estaba escrito, o si la palabra era ‚Äúcorrupto‚Äù o ‚Äúcorrupci√≥n‚Äù; con las formas base el algoritmo puede ‚Äúaprender‚Äù una idea general.&lt;/p&gt;

&lt;p&gt;Esta misma idea se puede aplicar para tokens muy raros dentro de nuestro texto‚Ä¶ podr√≠amos eliminar tokens que no aparezcan m√°s de $X$ cantidad de veces, bajo la sospecha de que tal vez fueron errores ortogr√°ficos o palabras sin importancia.&lt;/p&gt;

&lt;p&gt;Si tienes m√°s dudas o comentarios, no olvides hac√©rmelas llegar a trav√©s de los comentarios en esta p√°gina, o, para mayor seguridad, a mi cuenta de Twitter &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;y-las-visualizaciones&quot;&gt;¬øY las visualizaciones?&lt;/h3&gt;
&lt;p&gt;Para no fallarle al p√∫blico de Tacos de Datos, vamos a hacer una sencill√≠sima visualizaci√≥n con las palabras que m√°s frecuente usa el presidente mexicano en sus conferencias (ojo, ac√° no vamos a hablar de pol√≠tica). Y de paso aprovecho para presentarles un dataset que estoy manteniendo en Kaggle, en el cual coloco m√°s o menos semanalmente todas las versiones estenogr√°ficas de las conferencias que publica la presidencia de M√©xico. Puedes acceder al dataset aqu√≠ &lt;a href=&quot;https://www.kaggle.com/ioexception/mananeras&quot;&gt;Conferencias Ma√±aneras&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;El primer paso es crear una funci√≥n que nos ayude a tokenizar texto, esta es una funci√≥n que debemos conservar, ya que si en el futuro queremos repetir nuestros experimentos, debemos usar la misma forma de tokenizar para que exista consistencia en nuestros resultados.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.corpus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spanish_stopwords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spanish'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsed_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_punct&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_stop&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spanish_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Despu√©s, con todas nuestras frases cargadas en el arreglo &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lopez_obrador&lt;/code&gt; rellenamos un &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Counter&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amlo_phrase&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lopez_obrador&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amlo_phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Una vez que tenemos el objeto &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word_counter&lt;/code&gt; con los valores, podemos graficarlo:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Las 20 m√°s frecuentemente usadas&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;por L√≥pez Obrador en sus discursos&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;assets/detrasdelavis/003_amlo_words.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Algunas cosas esperadas y otras no tanto: habr√° que ver por qu√© se la pasa diciendo ‚Äúmil‚Äù y ‚Äúcasar‚Äù‚Ä¶ habr√° que ver tambi√©n si estas palabras no son sino las formas base de otras que tal vez hagan m√°s sentido (puede que al lematizar nuestros tokens estemos perdiendo esta informaci√≥n). Como dije, esto es solo el comienzo del an√°lisis de texto, ¬°a√∫n falta mucho por aprender!&lt;/p&gt;

&lt;p&gt;Si tienes alguna duda con lo presentado en este post, repito: preg√∫ntame por Twitter en &lt;a href=&quot;https://twitter.com/io_exception&quot;&gt;@io_exception&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Antonio Feregrino Bola√±os</name>
        
        
      </author>

      

      
        <category term="python" />
      
        <category term="spacy" />
      
        <category term="texto" />
      

      
        <summary type="html">¬øAlguna vez has querido analizar texto? en este post voy a tratar de explicar cu√°les son los primeros pasos recomendados para comenzar cualquier proyecto que involucre texto y ciencia de datos.</summary>
      

      
      
    </entry>
  
</feed>
